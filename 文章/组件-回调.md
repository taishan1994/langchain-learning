## åŸºæœ¬ä»‹ç»

LangChainæä¾›äº†ä¸€ä¸ªå›è°ƒç³»ç»Ÿï¼Œå¯ä»¥è®©ä½ æŒ‚åˆ°LLMåº”ç”¨ç¨‹åºçš„å„ä¸ªé˜¶æ®µã€‚è¿™å¯¹è®°å½•ã€ç›‘æ§ã€æµåª’ä½“å’Œå…¶ä»–ä»»åŠ¡å¾ˆæœ‰ç”¨ã€‚

ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨æ•´ä¸ªAPIä¸­çš„callbackså‚æ•°æ¥è®¢é˜…è¿™äº›äº‹ä»¶ã€‚è¿™ä¸ªå‚æ•°æ˜¯å¤„ç†ç¨‹åºå¯¹è±¡çš„åˆ—è¡¨ï¼Œè¿™äº›å¯¹è±¡åº”è¯¥å®ç°ä¸‹é¢è¯¦ç»†æè¿°çš„ä¸€ä¸ªæˆ–å¤šä¸ªæ–¹æ³•ã€‚

### å›è°ƒå¤„ç†ç¨‹åº

CallbackHandlersæ˜¯å®ç°CallbackHandleræ¥å£çš„å¯¹è±¡ï¼Œå®ƒå¯¹æ¯ä¸ªå¯ä»¥è®¢é˜…çš„äº‹ä»¶éƒ½æœ‰ä¸€ä¸ªæ–¹æ³•ã€‚å½“äº‹ä»¶è¢«è§¦å‘æ—¶ï¼ŒCallbackManagerå°†åœ¨æ¯ä¸ªå¤„ç†ç¨‹åºä¸Šè°ƒç”¨ç›¸åº”çš„æ–¹æ³•ã€‚

```python
class BaseCallbackHandler:
    """Base callback handler that can be used to handle callbacks from langchain."""

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """Run when LLM starts running."""

    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any
    ) -> Any:
        """Run when Chat Model starts running."""

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        """Run on new LLM token. Only available when streaming is enabled."""

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """Run when LLM ends running."""

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when LLM errors."""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        """Run when chain starts running."""

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:
        """Run when chain ends running."""

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when chain errors."""

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        """Run when tool starts running."""

    def on_tool_end(self, output: str, **kwargs: Any) -> Any:
        """Run when tool ends running."""

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when tool errors."""

    def on_text(self, text: str, **kwargs: Any) -> Any:
        """Run on arbitrary text."""

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        """Run on agent action."""

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:
        """Run on agent end."""
```

### å¼€å§‹

LangChainæä¾›äº†ä¸€äº›å†…ç½®çš„å¤„ç†ç¨‹åºï¼Œä½ å¯ä»¥ç”¨å®ƒä»¬æ¥å…¥é—¨ã€‚è¿™äº›éƒ½å¯ä»¥åœ¨langchain/callbacksæ¨¡å—ä¸­æ‰¾åˆ°ã€‚æœ€åŸºæœ¬çš„å¤„ç†ç¨‹åºæ˜¯StdOutCallbackHandlerï¼Œå®ƒåªæ˜¯å°†æ‰€æœ‰äº‹ä»¶è®°å½•åˆ°stdoutã€‚

è¯·æ³¨æ„ï¼Œå½“å¯¹è±¡ä¸Šçš„verboseæ ‡å¿—è¢«è®¾ç½®ä¸º "true "æ—¶ï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®ä¼ å…¥ï¼ŒStdOutCallbackHandlerä¹Ÿä¼šè¢«è°ƒç”¨ã€‚

```python
from langchain.callbacks import StdOutCallbackHandler
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

handler = StdOutCallbackHandler()
llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

# Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])
chain.run(number=2)

# Use verbose flag: Then, let's use the `verbose` flag to achieve the same result
chain = LLMChain(llm=llm, prompt=prompt, verbose=True)
chain.run(number=2)

# Request callbacks: Finally, let's use the request `callbacks` to achieve the same result
chain = LLMChain(llm=llm, prompt=prompt)
chain.run(number=2, callbacks=[handler])


    > Entering new LLMChain chain...
    Prompt after formatting:
    1 + 2 = 
    
    > Finished chain.
    
    
    > Entering new LLMChain chain...
    Prompt after formatting:
    1 + 2 = 
    
    > Finished chain.
    
    
    > Entering new LLMChain chain...
    Prompt after formatting:
    1 + 2 = 
    
    > Finished chain.


    '\n\n3'
```

### åœ¨å“ªé‡Œä¼ å…¥å›è°ƒ

å›è°ƒå‚æ•°åœ¨æ•´ä¸ªAPIçš„å¤§å¤šæ•°å¯¹è±¡ï¼ˆé“¾ã€æ¨¡å‹ã€å·¥å…·ã€ä»£ç†ç­‰ï¼‰ä¸Šæœ‰ä¸¤ä¸ªä¸åŒçš„åœ°æ–¹ï¼š

- æ„é€ å‡½æ•°å›è°ƒï¼šåœ¨æ„é€ å‡½æ•°ä¸­å®šä¹‰ï¼Œä¾‹å¦‚LLMChain(callbacks=[handler], tags=['a-tag'])ï¼Œå®ƒå°†è¢«ç”¨äºå¯¹è¯¥å¯¹è±¡çš„æ‰€æœ‰è°ƒç”¨ï¼Œå¹¶ä¸”å°†åªé’ˆå¯¹è¯¥å¯¹è±¡ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ å‘LLMChainæ„é€ å‡½æ•°ä¼ é€’ä¸€ä¸ªhandlerï¼Œå®ƒå°†ä¸ä¼šè¢«é™„å±äºè¯¥é“¾çš„Modelä½¿ç”¨ã€‚
- è¯·æ±‚å›è°ƒï¼šå®šä¹‰åœ¨ç”¨äºå‘å‡ºè¯·æ±‚çš„call()/run()/apply()æ–¹æ³•ä¸­ï¼Œä¾‹å¦‚chain.call(inputs, callbacks=[handler])ï¼Œå®ƒå°†ä»…ç”¨äºè¯¥ç‰¹å®šè¯·æ±‚ï¼Œä»¥åŠå®ƒåŒ…å«çš„æ‰€æœ‰å­è¯·æ±‚ï¼ˆä¾‹å¦‚ï¼Œå¯¹LLMChainçš„è°ƒç”¨ä¼šè§¦å‘å¯¹Modelçš„è°ƒç”¨ï¼Œè¯¥Modelä½¿ç”¨call()æ–¹æ³•ä¸­ä¼ é€’çš„ç›¸åŒhandlerï¼‰ã€‚

verboseå‚æ•°åœ¨æ•´ä¸ªAPIçš„å¤§å¤šæ•°å¯¹è±¡ï¼ˆé“¾ã€æ¨¡å‹ã€å·¥å…·ã€ä»£ç†ç­‰ï¼‰ä¸Šéƒ½å¯ä»¥ä½œä¸ºæ„é€ å‚æ•°ä½¿ç”¨ï¼Œä¾‹å¦‚LLMChain(verbose=True)ï¼Œå®ƒç›¸å½“äºå°†ConsoleCallbackHandlerä¼ é€’ç»™è¯¥å¯¹è±¡å’Œæ‰€æœ‰å­å¯¹è±¡çš„callbackså‚æ•°ã€‚è¿™å¯¹è°ƒè¯•å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºå®ƒå°†æŠŠæ‰€æœ‰äº‹ä»¶è®°å½•åˆ°æ§åˆ¶å°ã€‚

### ä½ æƒ³åœ¨ä»€ä¹ˆæ—¶å€™ä½¿ç”¨è¿™äº›ä¸œè¥¿å‘¢ï¼Ÿ
- æ„é€ å‡½æ•°å›è°ƒå¯¹è¯¸å¦‚æ—¥å¿—ã€ç›‘æ§ç­‰ç”¨ä¾‹æœ€æœ‰ç”¨ï¼Œè¿™äº›ç”¨ä¾‹ä¸æ˜¯é’ˆå¯¹å•ä¸ªè¯·æ±‚ï¼Œè€Œæ˜¯é’ˆå¯¹æ•´ä¸ªé“¾ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³è®°å½•æ‰€æœ‰å‘LLMChainå‘å‡ºçš„è¯·æ±‚ï¼Œä½ å¯ä»¥å‘æ„é€ å‡½æ•°ä¼ é€’ä¸€ä¸ªå¤„ç†ç¨‹åºã€‚
- è¯·æ±‚å›è°ƒå¯¹æµåª’ä½“ç­‰ç”¨ä¾‹æœ€æœ‰ç”¨ï¼Œä½ æƒ³æŠŠå•ä¸ªè¯·æ±‚çš„è¾“å‡ºæµå‘ç‰¹å®šçš„websocketè¿æ¥ï¼Œæˆ–å…¶ä»–ç±»ä¼¼ç”¨ä¾‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³æŠŠå•ä¸ªè¯·æ±‚çš„è¾“å‡ºæµåˆ°ä¸€ä¸ªwebsocketï¼Œä½ ä¼šæŠŠä¸€ä¸ªå¤„ç†ç¨‹åºä¼ é€’ç»™call()æ–¹æ³•

## å¼‚æ­¥å›è°ƒ

å¦‚æœä½ æ‰“ç®—ä½¿ç”¨å¼‚æ­¥APIï¼Œå»ºè®®ä½¿ç”¨AsyncCallbackHandlerä»¥é¿å…é˜»å¡è¿è¡Œå¾ªç¯ã€‚

å¦‚æœä½ åœ¨ä½¿ç”¨å¼‚æ­¥æ–¹æ³•è¿è¡Œä½ çš„llm/chain/tool/agentæ—¶ä½¿ç”¨åŒæ­¥CallbackHandlerï¼Œå®ƒä»ç„¶å¯ä»¥å·¥ä½œã€‚ç„¶è€Œï¼Œåœ¨å¼•æ“ç›–ä¸‹ï¼Œå®ƒå°†è¢«run_in_executorè°ƒç”¨ï¼Œå¦‚æœä½ çš„CallbackHandlerä¸æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œå°±ä¼šå¯¼è‡´é—®é¢˜ã€‚

```python
import asyncio
from typing import Any, Dict, List

from langchain.chat_models import ChatOpenAI
from langchain.schema import LLMResult, HumanMessage
from langchain.callbacks.base import AsyncCallbackHandler, BaseCallbackHandler


class MyCustomSyncHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"Sync handler being called in a `thread_pool_executor`: token: {token}")


class MyCustomAsyncHandler(AsyncCallbackHandler):
    """Async callback handler that can be used to handle callbacks from langchain."""

    async def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when chain starts running."""
        print("zzzz....")
        await asyncio.sleep(0.3)
        class_name = serialized["name"]
        print("Hi! I just woke up. Your llm is starting")

    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when chain ends running."""
        print("zzzz....")
        await asyncio.sleep(0.3)
        print("Hi! I just woke up. Your llm is ending")


# To enable streaming, we pass in `streaming=True` to the ChatModel constructor
# Additionally, we pass in a list with our custom handler
chat = ChatOpenAI(
    max_tokens=25,
    streaming=True,
    callbacks=[MyCustomSyncHandler(), MyCustomAsyncHandler()],
)

await chat.agenerate([[HumanMessage(content="Tell me a joke")]])


    zzzz....
    Hi! I just woke up. Your llm is starting
    Sync handler being called in a `thread_pool_executor`: token: 
    Sync handler being called in a `thread_pool_executor`: token: Why
    Sync handler being called in a `thread_pool_executor`: token:  don
    Sync handler being called in a `thread_pool_executor`: token: 't
    Sync handler being called in a `thread_pool_executor`: token:  scientists
    Sync handler being called in a `thread_pool_executor`: token:  trust
    Sync handler being called in a `thread_pool_executor`: token:  atoms
    Sync handler being called in a `thread_pool_executor`: token: ?
    Sync handler being called in a `thread_pool_executor`: token:  
    
    
    Sync handler being called in a `thread_pool_executor`: token: Because
    Sync handler being called in a `thread_pool_executor`: token:  they
    Sync handler being called in a `thread_pool_executor`: token:  make
    Sync handler being called in a `thread_pool_executor`: token:  up
    Sync handler being called in a `thread_pool_executor`: token:  everything
    Sync handler being called in a `thread_pool_executor`: token: .
    Sync handler being called in a `thread_pool_executor`: token: 
    zzzz....
    Hi! I just woke up. Your llm is ending





    LLMResult(generations=[[ChatGeneration(text="Why don't scientists trust atoms? \n\nBecause they make up everything.", generation_info=None, message=AIMessage(content="Why don't scientists trust atoms? \n\nBecause they make up everything.", additional_kwargs={}, example=False))]], llm_output={'token_usage': {}, 'model_name': 'gpt-3.5-turbo'})

```

## å®šåˆ¶å›è°ƒå¤„ç†å™¨

ä½ ä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„å¤„ç†ç¨‹åºæ¥è®¾ç½®åœ¨è¯¥å¯¹è±¡ä¸Šã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ç”¨ä¸€ä¸ªè‡ªå®šä¹‰å¤„ç†ç¨‹åºæ¥å®ç°æµåª’ä½“ã€‚

```python
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage


class MyCustomHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        print(f"My custom handler, token: {token}")


# To enable streaming, we pass in `streaming=True` to the ChatModel constructor
# Additionally, we pass in a list with our custom handler
chat = ChatOpenAI(max_tokens=25, streaming=True, callbacks=[MyCustomHandler()])

chat([HumanMessage(content="Tell me a joke")])

    My custom handler, token: 
    My custom handler, token: Why
    My custom handler, token:  don
    My custom handler, token: 't
    My custom handler, token:  scientists
    My custom handler, token:  trust
    My custom handler, token:  atoms
    My custom handler, token: ?
    My custom handler, token:  
    
    
    My custom handler, token: Because
    My custom handler, token:  they
    My custom handler, token:  make
    My custom handler, token:  up
    My custom handler, token:  everything
    My custom handler, token: .
    My custom handler, token: 





    AIMessage(content="Why don't scientists trust atoms? \n\nBecause they make up everything.", additional_kwargs={}, example=False)

```

## å®šåˆ¶é“¾ä¸­ä½¿ç”¨å›è°ƒ

å½“ä½ åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰é“¾æ—¶ï¼Œä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°å°†å…¶è®¾ç½®ä¸ºä½¿ç”¨ä¸æ‰€æœ‰å†…ç½®é“¾ç›¸åŒçš„å›è°ƒç³»ç»Ÿã€‚é“¾/LLM/èŠå¤©æ¨¡å‹/ä»£ç†/å·¥å…·ä¸Šçš„_callã€_generateã€_runå’Œç­‰æ•ˆçš„å¼‚æ­¥æ–¹æ³•ç°åœ¨ä¼šæ¥æ”¶ä¸€ä¸ªåä¸ºrun_managerçš„ç¬¬äºŒä¸ªå‚æ•°ï¼Œè¯¥å‚æ•°ä¸è¯¥è¿è¡Œç»‘å®šï¼Œå¹¶åŒ…å«è¯¥å¯¹è±¡å¯ä½¿ç”¨çš„æ—¥å¿—æ–¹æ³•ï¼ˆå³on_llm_new_tokenï¼‰ã€‚è¿™åœ¨æ„å»ºè‡ªå®šä¹‰é“¾çš„æ—¶å€™å¾ˆæœ‰ç”¨ã€‚å…³äºå¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰é“¾å¹¶åœ¨å…¶ä¸­ä½¿ç”¨å›è°ƒçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§æœ¬æŒ‡å—ã€‚https://python.langchain.com/docs/modules/chains/how_to/custom_chain.html

## è®°å½•æ—¥å¿—åˆ°æ–‡ä»¶

è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•æ‰“å°æ—¥å¿—åˆ°æ–‡ä»¶ã€‚å®ƒå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨FileCallbackHandlerï¼Œå®ƒä¸StdOutCallbackHandleråšåŒæ ·çš„äº‹æƒ…ï¼Œä½†å´å°†è¾“å‡ºå†™å…¥æ–‡ä»¶ã€‚å®ƒè¿˜ä½¿ç”¨loguruåº“æ¥è®°å½•å¤„ç†ç¨‹åºæœªæ•è·çš„å…¶ä»–è¾“å‡ºã€‚

```python
rom loguru import logger

from langchain.callbacks import FileCallbackHandler
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

logfile = "output.log"

logger.add(logfile, colorize=True, enqueue=True)
handler = FileCallbackHandler(logfile)

llm = OpenAI()
prompt = PromptTemplate.from_template("1 + {number} = ")

# this chain will both print to stdout (because verbose=True) and write to 'output.log'
# if verbose=False, the FileCallbackHandler will still write to 'output.log'
chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=True)
answer = chain.run(number=2)
logger.info(answer)


    
    
    > Entering new LLMChain chain...
    Prompt after formatting:
    1 + 2 = 


    [32m2023-06-01 18:36:38.929[0m | [1mINFO    [0m | [36m__main__[0m:[36m<module>[0m:[36m20[0m - [1m
    
    3[0m


    
    > Finished chain.
```

æˆ‘ä»¬å¯ä»¥æ‰“å¼€output.logæ–‡ä»¶ï¼Œçœ‹åˆ°è¾“å‡ºå·²ç»è¢«æ•è·ã€‚

```python
pip install ansi2html > /dev/null

from IPython.display import display, HTML
from ansi2html import Ansi2HTMLConverter

with open("output.log", "r") as f:
    content = f.read()

conv = Ansi2HTMLConverter()
html = conv.convert(content, full=True)

display(HTML(html))

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title></title>
<style type="text/css">
.ansi2html-content { display: inline; white-space: pre-wrap; word-wrap: break-word; }
.body_foreground { color: #AAAAAA; }
.body_background { background-color: #000000; }
.inv_foreground { color: #000000; }
.inv_background { background-color: #AAAAAA; }
.ansi1 { font-weight: bold; }
.ansi3 { font-style: italic; }
.ansi32 { color: #00aa00; }
.ansi36 { color: #00aaaa; }
</style>
</head>
<body class="body_foreground body_background" style="font-size: normal;" >
<pre class="ansi2html-content">


<span class="ansi1">&gt; Entering new LLMChain chain...</span>
Prompt after formatting:
<span class="ansi1 ansi32"></span><span class="ansi1 ansi3 ansi32">1 + 2 = </span>

<span class="ansi1">&gt; Finished chain.</span>
<span class="ansi32">2023-06-01 18:36:38.929</span> | <span class="ansi1">INFO    </span> | <span class="ansi36">__main__</span>:<span class="ansi36">&lt;module&gt;</span>:<span class="ansi36">20</span> - <span class="ansi1">

3</span>

</pre>
</body>

</html>

```

## å¤šä¸ªå›è°ƒå¤„ç†ç¨‹åº

åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨callbacks=åœ¨åˆ›å»ºå¯¹è±¡æ—¶ä¼ å…¥å›è°ƒå¤„ç†ç¨‹åºã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›è°ƒå°†è¢«æ‰©å±•åˆ°è¯¥ç‰¹å®šå¯¹è±¡ã€‚

ç„¶è€Œï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œåœ¨è¿è¡Œå¯¹è±¡æ—¶ä¼ å…¥å¤„ç†ç¨‹åºåè€Œæ˜¯æœ‰åˆ©çš„ã€‚å½“æˆ‘ä»¬åœ¨æ‰§è¡Œä¸€ä¸ªè¿è¡Œæ—¶ä½¿ç”¨callbackså…³é”®å­—argä¼ é€’CallbackHandlersæ—¶ï¼Œè¿™äº›å›è°ƒå°†ç”±æ‰€æœ‰å‚ä¸æ‰§è¡Œçš„åµŒå¥—å¯¹è±¡å‘å‡ºã€‚ä¾‹å¦‚ï¼Œå½“ä¸€ä¸ªå¤„ç†ç¨‹åºè¢«ä¼ é€’ç»™ä¸€ä¸ªä»£ç†æ—¶ï¼Œå®ƒå°†è¢«ç”¨äºæ‰€æœ‰ä¸ä»£ç†æœ‰å…³çš„å›è°ƒï¼Œä»¥åŠæ‰€æœ‰å‚ä¸ä»£ç†æ‰§è¡Œçš„å¯¹è±¡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå·¥å…·ã€LLMChainå’ŒLLMã€‚

è¿™å°±é¿å…äº†æˆ‘ä»¬æ‰‹åŠ¨å°†å¤„ç†ç¨‹åºé™„åŠ åˆ°æ¯ä¸ªå•ç‹¬çš„åµŒå¥—å¯¹è±¡ã€‚

```python
from typing import Dict, Union, Any, List

from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import AgentAction
from langchain.agents import AgentType, initialize_agent, load_tools
from langchain.callbacks import tracing_enabled
from langchain.llms import OpenAI


# First, define custom callback handler implementations
class MyCustomHandlerOne(BaseCallbackHandler):
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        print(f"on_llm_start {serialized['name']}")

    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:
        print(f"on_new_token {token}")

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> Any:
        """Run when LLM errors."""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        print(f"on_chain_start {serialized['name']}")

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        print(f"on_tool_start {serialized['name']}")

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        print(f"on_agent_action {action}")


class MyCustomHandlerTwo(BaseCallbackHandler):
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        print(f"on_llm_start (I'm the second handler!!) {serialized['name']}")


# Instantiate the handlers
handler1 = MyCustomHandlerOne()
handler2 = MyCustomHandlerTwo()

# Setup the agent. Only the `llm` will issue callbacks for handler2
llm = OpenAI(temperature=0, streaming=True, callbacks=[handler2])
tools = load_tools(["llm-math"], llm=llm)
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

# Callbacks for handler1 will be issued by every object involved in the
# Agent execution (llm, llmchain, tool, agent executor)
agent.run("What is 2 raised to the 0.235 power?", callbacks=[handler1])

    on_chain_start AgentExecutor
    on_chain_start LLMChain
    on_llm_start OpenAI
    on_llm_start (I'm the second handler!!) OpenAI
    on_new_token  I
    on_new_token  need
    on_new_token  to
    on_new_token  use
    on_new_token  a
    on_new_token  calculator
    on_new_token  to
    on_new_token  solve
    on_new_token  this
    on_new_token .
    on_new_token 
    Action
    on_new_token :
    on_new_token  Calculator
    on_new_token 
    Action
    on_new_token  Input
    on_new_token :
    on_new_token  2
    on_new_token ^
    on_new_token 0
    on_new_token .
    on_new_token 235
    on_new_token 
    on_agent_action AgentAction(tool='Calculator', tool_input='2^0.235', log=' I need to use a calculator to solve this.\nAction: Calculator\nAction Input: 2^0.235')
    on_tool_start Calculator
    on_chain_start LLMMathChain
    on_chain_start LLMChain
    on_llm_start OpenAI
    on_llm_start (I'm the second handler!!) OpenAI
    on_new_token 
    on_new_token ```text
    on_new_token 
    
    on_new_token 2
    on_new_token **
    on_new_token 0
    on_new_token .
    on_new_token 235
    on_new_token 
    
    on_new_token ```
    
    on_new_token ...
    on_new_token num
    on_new_token expr
    on_new_token .
    on_new_token evaluate
    on_new_token ("
    on_new_token 2
    on_new_token **
    on_new_token 0
    on_new_token .
    on_new_token 235
    on_new_token ")
    on_new_token ...
    on_new_token 
    
    on_new_token 
    on_chain_start LLMChain
    on_llm_start OpenAI
    on_llm_start (I'm the second handler!!) OpenAI
    on_new_token  I
    on_new_token  now
    on_new_token  know
    on_new_token  the
    on_new_token  final
    on_new_token  answer
    on_new_token .
    on_new_token 
    Final
    on_new_token  Answer
    on_new_token :
    on_new_token  1
    on_new_token .
    on_new_token 17
    on_new_token 690
    on_new_token 67
    on_new_token 372
    on_new_token 187
    on_new_token 674
    on_new_token 





    '1.1769067372187674'

```

## æ ‡ç­¾

ä½ å¯ä»¥é€šè¿‡å‘call()/run()/apply()æ–¹æ³•ä¼ é€’ä¸€ä¸ªæ ‡ç­¾å‚æ•°æ¥ä¸ºä½ çš„å›è°ƒæ·»åŠ æ ‡ç­¾ã€‚è¿™å¯¹äºè¿‡æ»¤ä½ çš„æ—¥å¿—å¾ˆæœ‰ç”¨ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³è®°å½•æ‰€æœ‰å¯¹ç‰¹å®šLLMChainçš„è¯·æ±‚ï¼Œä½ å¯ä»¥æ·»åŠ ä¸€ä¸ªæ ‡ç­¾ï¼Œç„¶åé€šè¿‡è¯¥æ ‡ç­¾è¿‡æ»¤ä½ çš„æ—¥å¿—ã€‚ä½ å¯ä»¥å°†æ ‡ç­¾ä¼ é€’ç»™æ„é€ å‡½æ•°å’Œè¯·æ±‚å›è°ƒï¼Œè¯¦æƒ…è¯·è§ä¸Šé¢çš„ä¾‹å­ã€‚è¿™äº›æ ‡ç­¾ç„¶åè¢«ä¼ é€’åˆ° "å¼€å§‹ "å›è°ƒæ–¹æ³•çš„æ ‡ç­¾å‚æ•°ä¸­ï¼Œå³on_llm_startã€on_chat_model_startã€on_chain_startã€on_tool_startã€‚

## tokenè®¡æ•°

LangChainæä¾›äº†ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå¯ä»¥è®©ä½ è®¡ç®—ä»¤ç‰Œã€‚

```python
import asyncio

from langchain.callbacks import get_openai_callback
from langchain.llms import OpenAI

llm = OpenAI(temperature=0)
with get_openai_callback() as cb:
    llm("What is the square root of 4?")

total_tokens = cb.total_tokens
assert total_tokens > 0

with get_openai_callback() as cb:
    llm("What is the square root of 4?")
    llm("What is the square root of 4?")

assert cb.total_tokens == total_tokens * 2

# You can kick off concurrent runs from within the context manager
with get_openai_callback() as cb:
    await asyncio.gather(
        *[llm.agenerate(["What is the square root of 4?"]) for _ in range(3)]
    )

assert cb.total_tokens == total_tokens * 3

# The context manager is concurrency safe
task = asyncio.create_task(llm.agenerate(["What is the square root of 4?"]))
with get_openai_callback() as cb:
    await llm.agenerate(["What is the square root of 4?"])

await task
assert cb.total_tokens == total_tokens
```

## è¿½è¸ª

æœ‰ä¸¤ç§æ¨èçš„æ–¹æ³•æ¥è¿½è¸ªä½ çš„LangChainsï¼š

- å°†LANGCHAIN_TRACINGç¯å¢ƒå˜é‡è®¾ç½®ä¸º "true"ã€‚
- ä½¿ç”¨å¸¦æœ‰tracing_enabled()çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥è¿½è¸ªæŸä¸ªç‰¹å®šçš„ä»£ç å—ã€‚

æ³¨æ„ï¼Œå¦‚æœç¯å¢ƒå˜é‡è¢«è®¾ç½®ï¼Œæ‰€æœ‰çš„ä»£ç éƒ½ä¼šè¢«è¿½è¸ªï¼Œä¸ç®¡å®ƒæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ã€‚

```python
import os

from langchain.agents import AgentType, initialize_agent, load_tools
from langchain.callbacks import tracing_enabled
from langchain.llms import OpenAI

# To run the code, make sure to set OPENAI_API_KEY and SERPAPI_API_KEY
llm = OpenAI(temperature=0)
tools = load_tools(["llm-math", "serpapi"], llm=llm)
agent = initialize_agent(
    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True
)

questions = [
    "Who won the US Open men's final in 2019? What is his age raised to the 0.334 power?",
    "Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?",
    "Who won the most recent formula 1 grand prix? What is their age raised to the 0.23 power?",
    "Who won the US Open women's final in 2019? What is her age raised to the 0.34 power?",
    "Who is Beyonce's husband? What is his age raised to the 0.19 power?",
]
os.environ["LANGCHAIN_TRACING"] = "true"

# Both of the agent runs will be traced because the environment variable is set
agent.run(questions[0])
with tracing_enabled() as session:
    assert session
    agent.run(questions[1])

    WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f8b36d0>: Failed to establish a new connection: [Errno 61] Connection refused'))


    
    
    > Entering new AgentExecutor chain...
     I need to find out who won the US Open men's final in 2019 and then calculate his age raised to the 0.334 power.
    Action: Search
    Action Input: "US Open men's final 2019 winner"
    Observation: Rafael Nadal defeated Daniil Medvedev in the final, 7â€“5, 6â€“3, 5â€“7, 4â€“6, 6â€“4 to win the men's singles tennis title at the 2019 US Open. It was his fourth US ...
    Thought: I need to find out the age of the winner
    Action: Search
    Action Input: "Rafael Nadal age"
    Observation: 37 years
    Thought: I now need to calculate the age raised to the 0.334 power
    Action: Calculator
    Action Input: 37^0.334
    Observation: Answer: 3.340253100876781
    Thought:

    WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f8c0f50>: Failed to establish a new connection: [Errno 61] Connection refused'))
    WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f8e6f50>: Failed to establish a new connection: [Errno 61] Connection refused'))


     I now know the final answer
    Final Answer: Rafael Nadal, aged 37, won the US Open men's final in 2019 and his age raised to the 0.334 power is 3.340253100876781.
    
    > Finished chain.
    
    
    > Entering new AgentExecutor chain...
     I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.
    Action: Search
    Action Input: "Olivia Wilde boyfriend"
    Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.
    Thought: I need to find out Harry Styles' age.
    Action: Search
    Action Input: "Harry Styles age"
    Observation: 29 years
    Thought: I need to calculate 29 raised to the 0.23 power.
    Action: Calculator
    Action Input: 29^0.23
    Observation: Answer: 2.169459462491557
    Thought:

    WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f8fa590>: Failed to establish a new connection: [Errno 61] Connection refused'))


     I now know the final answer.
    Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.
    
    > Finished chain.


# Now, we unset the environment variable and use a context manager.

if "LANGCHAIN_TRACING" in os.environ:
    del os.environ["LANGCHAIN_TRACING"]

# here, we are writing traces to "my_test_session"
with tracing_enabled("my_test_session") as session:
    assert session
    agent.run(questions[0])  # this should be traced

agent.run(questions[1])  # this should not be traced

    WARNING:root:Failed to load my_test_session session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=my_test_session (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f8e41d0>: Failed to establish a new connection: [Errno 61] Connection refused'))


    
    
    > Entering new AgentExecutor chain...
     I need to find out who won the US Open men's final in 2019 and then calculate his age raised to the 0.334 power.
    Action: Search
    Action Input: "US Open men's final 2019 winner"
    Observation: Rafael Nadal defeated Daniil Medvedev in the final, 7â€“5, 6â€“3, 5â€“7, 4â€“6, 6â€“4 to win the men's singles tennis title at the 2019 US Open. It was his fourth US ...
    Thought: I need to find out the age of the winner
    Action: Search
    Action Input: "Rafael Nadal age"
    Observation: 37 years
    Thought: I now need to calculate the age raised to the 0.334 power
    Action: Calculator
    Action Input: 37^0.334
    Observation: Answer: 3.340253100876781
    Thought:

    WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f8d0a50>: Failed to establish a new connection: [Errno 61] Connection refused'))


     I now know the final answer
    Final Answer: Rafael Nadal, aged 37, won the US Open men's final in 2019 and his age raised to the 0.334 power is 3.340253100876781.
    
    > Finished chain.
    
    
    > Entering new AgentExecutor chain...
     I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.
    Action: Search
    Action Input: "Olivia Wilde boyfriend"
    Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.
    Thought: I need to find out Harry Styles' age.
    Action: Search
    Action Input: "Harry Styles age"
    Observation: 29 years
    Thought: I need to calculate 29 raised to the 0.23 power.
    Action: Calculator
    Action Input: 29^0.23
    Observation: Answer: 2.169459462491557
    Thought: I now know the final answer.
    Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.
    
    > Finished chain.





    "Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557."


import asyncio

# The context manager is concurrency safe:
if "LANGCHAIN_TRACING" in os.environ:
    del os.environ["LANGCHAIN_TRACING"]

# start a background task
task = asyncio.create_task(agent.arun(questions[0]))  # this should not be traced
with tracing_enabled() as session:
    assert session
    tasks = [agent.arun(q) for q in questions[1:3]]  # these should be traced
    await asyncio.gather(*tasks)

await task

    WARNING:root:Failed to load default session, using empty session: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /sessions?name=default (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f916ed0>: Failed to establish a new connection: [Errno 61] Connection refused'))


    
    
    > Entering new AgentExecutor chain...
    
    
    > Entering new AgentExecutor chain...
    
    
    > Entering new AgentExecutor chain...
     I need to find out who Olivia Wilde's boyfriend is and then calculate his age raised to the 0.23 power.
    Action: Search
    Action Input: "Olivia Wilde boyfriend" I need to find out who won the grand prix and then calculate their age raised to the 0.23 power.
    Action: Search
    Action Input: "Formula 1 Grand Prix Winner" I need to find out who won the US Open men's final in 2019 and then calculate his age raised to the 0.334 power.
    Action: Search
    Action Input: "US Open men's final 2019 winner"
    Observation: Sudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.
    Thought:
    Observation: Rafael Nadal defeated Daniil Medvedev in the final, 7â€“5, 6â€“3, 5â€“7, 4â€“6, 6â€“4 to win the men's singles tennis title at the 2019 US Open. It was his fourth US ...
    Thought:
    Observation: The first Formula One World Drivers' Champion was Giuseppe Farina in the 1950 championship and the current title holder is Max Verstappen in the 2022 season.
    Thought: I need to find out Harry Styles' age.
    Action: Search
    Action Input: "Harry Styles age" I need to find out the age of the winner
    Action: Search
    Action Input: "Rafael Nadal age"
    Observation: 29 years
    Thought:
    Observation: 37 years
    Thought: I need to find out Max Verstappen's age.
    Action: Search
    Action Input: "Max Verstappen Age" I need to calculate 29 raised to the 0.23 power.
    Action: Calculator
    Action Input: 29^0.23 I now need to calculate the age raised to the 0.334 power
    Action: Calculator
    Action Input: 37^0.334
    Observation: Answer: 2.169459462491557
    Thought:
    Observation: 25 years
    Thought:
    Observation: Answer: 3.340253100876781
    Thought:

    WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f95dbd0>: Failed to establish a new connection: [Errno 61] Connection refused'))


     I now know the final answer.
    Final Answer: Harry Styles is Olivia Wilde's boyfriend and his current age raised to the 0.23 power is 2.169459462491557.
    
    > Finished chain.
     I need to calculate 25 raised to the 0.23 power.
    Action: Calculator
    Action Input: 25^0.23 I now know the final answer
    Final Answer: Rafael Nadal, aged 37, won the US Open men's final in 2019 and his age raised to the 0.334 power is 3.340253100876781.
    
    > Finished chain.
    
    Observation: Answer: 2.096651272316035
    Thought:

    WARNING:root:Failed to persist run: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /chain-runs (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x12f95de50>: Failed to establish a new connection: [Errno 61] Connection refused'))


     I now know the final answer.
    Final Answer: Max Verstappen, aged 25, won the most recent Formula 1 Grand Prix and his age raised to the 0.23 power is 2.096651272316035.
    
    > Finished chain.





    "Rafael Nadal, aged 37, won the US Open men's final in 2019 and his age raised to the 0.334 power is 3.340253100876781."

```



## é›†æˆ

ä¸€äº›ç›¸å…³çš„ä¾‹å­ï¼Œè¿™é‡Œä¸å±•å¼€äº†ï¼Œå¯è‡ªè¡ŒæŸ¥çœ‹ï¼šhttps://python.langchain.com/docs/modules/callbacks/integrations/

## æ€»ç»“

å›è°ƒä¸€èˆ¬æ˜¯åœ¨ç¨‹åºçš„æŸä¸ªé˜¶æ®µæ‰§è¡ŒæŸç§ç‰¹å®šçš„æ“ä½œï¼Œè¿™åœ¨è®¸å¤šä¸åŒçš„ç¨‹åºä¹‹ä¸­éƒ½å¾ˆæœ‰ç”¨ã€‚åˆ°è¿™é‡Œï¼Œå°±æŠŠlangchainæ–‡æ¡£ä¸­çš„åŸºæœ¬çŸ¥è¯†è¿‡äº†ä¸€éã€‚ä¸ä»…ä»…æ˜¯å¯¹äºlangchainï¼Œå…¶æ€æƒ³ä¹Ÿå¯ä»¥å€Ÿé‰´äºæˆ‘ä»¬è‡ªå·±å¼€å‘LLMç¨‹åºï¼Œæ¯”å¦‚æ•°æ®æ¥å…¥ã€promptç®¡ç†ã€å„ç§NLPä»»åŠ¡ã€æ™ºèƒ½ä½“é€‰æ‹©ä¸åŒçš„ä»»åŠ¡ã€é”™è¯¯å¤„ç†ã€æ—¥å¿—è¿½è¸ªã€å†å²ä¿¡æ¯è®°å½•ã€æ•°æ®è¾“å‡ºç­‰ã€‚

å­¦ä¹ åŸºç¡€çŸ¥è¯†æ˜¯æ¯ç‡¥çš„ï¼Œè€Œä¸”åŒ…å«çš„ä¸œè¥¿å¤ªå¤šäº†ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å®è·µä¸­äº†è§£å’Œå·©å›ºç›¸å…³çŸ¥è¯†ã€‚å½“ç„¶ï¼Œä¸Šè¿°éƒ½æ˜¯é’ˆå¯¹äºè‹±æ–‡çš„ï¼Œæˆ–è€…è¯´æ˜¯é’ˆå¯¹äºopenai chatgptçš„ï¼Œå¦‚ä½•å°†langchainåº”ç”¨åˆ°ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ä¸­ä¹Ÿæ˜¯æˆ‘ä»¬æ¥ä¸‹æ¥è¦å­¦ä¹ çš„ã€‚